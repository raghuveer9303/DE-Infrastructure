version: '3.8'

services:
  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  postgres:
    image: postgres:14
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_DB:-data_engineering}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-webserver:
    image: apache/airflow:2.7.1
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres/${POSTGRES_DB:-data_engineering}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
    volumes:
      - ./projects/*/airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    image: apache/airflow:2.7.1
    depends_on:
      - airflow-webserver
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres/${POSTGRES_DB:-data_engineering}
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW_FERNET_KEY:-}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./projects/*/airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
    command: scheduler

  jupyterhub:
    image: jupyterhub/jupyterhub:latest
    ports:
      - "8000:8000"
    volumes:
      - ./shared/notebooks:/home/jovyan/work
      - jupyterhub_data:/srv/jupyterhub
    environment:
      - JUPYTER_ENABLE_LAB=yes
    depends_on:
      - postgres

  superset:
    image: apache/superset:latest
    ports:
      - "8088:8088"
    environment:
      - SUPERSET_SECRET_KEY=${SUPERSET_SECRET_KEY:-}
      - POSTGRES_DB=${POSTGRES_DB:-data_engineering}
      - POSTGRES_USER=${POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
    depends_on:
      - postgres
    volumes:
      - superset_home:/app/superset_home

  spark-master:
    image: bitnami/spark:latest
    ports:
      - "4040:4040"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_IPS=spark-master
      - SPARK_PUBLIC_DNS=localhost
    volumes:
      - spark_data:/bitnami/spark

  spark-worker:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_IPS=spark-worker
      - SPARK_PUBLIC_DNS=localhost
    depends_on:
      - spark-master
    volumes:
      - spark_data:/bitnami/spark

  dbt:
    image: ghcr.io/dbt-labs/dbt-postgres:1.5.0
    volumes:
      - ./projects/*/dbt:/opt/dbt
    environment:
      - DBT_PROFILES_DIR=/opt/dbt
    depends_on:
      - postgres

volumes:
  minio_data:
  postgres_data:
  airflow_logs:
  jupyterhub_data:
  superset_home:
  spark_data: 